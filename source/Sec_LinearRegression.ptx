<section xml:id="Sec_LinearRegression">
  <title>Linear Regression</title>
  <subsection>
    <title>Philosophy of Linear Regression</title>
    <p>
      It turns out that it's fairly rare to find variables in the world that are truly independent of each other.
      The world is full of variables that have hidden connections and part of understanding the universe we exist in is uncovering all these connections.
      This is true since the first cave person stood near a fire and realized that standing near a fire made them warmer,
      thus uncovering a connection between standing near fires and warmth.
    </p>
    <p>
      However, since the world is an interconnected web of hidden influences,
      it's rare to see two variables that can be solely and completely determined by one another.
      In the example of our erstwhile caveperson,
      it's certain that standing near fire could make you warm,
      but so could standing in sunlight,
      so could wearing more furs.
      There are a lot of factors that go into determining warmth besides proximity to flames.
    </p>
    <p>
      Our goal here then is to take observations and determine whether or not they are connected,
      and what the connection is so we could potentially use one as a predictor for the other.
      But moreover,
      we also want to measure how strong this connection is so that we can asses how accurate these predictions might be,
      and what other variables may be influencing these outcomes.
    </p>
  </subsection>
  <subsection>
    <title>Basic Mechanics of Linear Regression</title>
    <p>
      Wee begin with an illustrative example.
    </p>
    <example xml:id="Example_BasicRegression">
      <title>Finding a best-fit line</title>
      <statement>
        <p>
          <term>Question:<nbsp/></term> Consider the data set:
          <me>
            \begin{array}{c|c} x\amp y\\ \hline 1\amp 2\\ 2\amp 2\\ 3\amp 4\\ 4\amp 4\\ 5\amp 6 \end{array}
          </me>
        </p>
        <p>
          If we were to plot this data, it would look like:
        </p>
        <image>
<latex-image>
<![CDATA[\begin{tikzpicture}[scale=.6][domain=-1:7]
\draw[gray!50, thin, step=1] (-1,-1) grid (7,7);
\draw[very thick,->] (-1,0) -- (7.1,0) node[right] {\(x\)};
\draw[very thick,->] (0,-1) -- (0,7.1) node[above] {\(y\)};
\draw[blue, fill] (1,2) circle (3pt);
\draw[blue, fill] (2,2) circle (3pt);
\draw[blue, fill] (3,4) circle (3pt);
\draw[blue, fill] (4,4) circle (3pt);
\draw[blue, fill] (5,6) circle (3pt);\end{tikzpicture}]]>
</latex-image>
        </image>
        <p>
          Our goal here is to find a linear relationship between <m>x</m> and <m>y</m>.
          Graphically that means we want to draw a line that best fits the data we present here.
          Of course, there is no line that can pass through all these points.
          Our goal is to find the line that best fits these points,
          and to measure how well this line predicts actual values from the data.
        </p>
      </statement>
    </example>
    <p>
      We now commence with listing the basic arithmetic mechanics for finding a regression line.
      Note that demonstrating why these formulations generate the best-fit line is beyond the scope of this text.
      The most important thing to remember is:
    </p>
    <p>
      <q>Given a bunch of points <m>(x_1, y_1), \ldots, (x_n, y_n)</m>,
      and a linear equation <m>\ell(x)=mx+b</m> the <term>error</term>
      of each point is <m>e_i=\ell(x_i)-y_i</m>,
      that is the difference between the actual <m>y</m> values,
      and what <m>\ell</m> predicts should bee the <m>y</m> values.
      We are trying to find <m>m,
      b</m> so that the sum of the errors squared <m>\sum e_i^2</m> is minimized.</q>
    </p>
    <p>
      Why minimize <m>\sum e_i^2</m>?
      Squaring the data erases the distinction between positive and negative error,
      between over and under shoot.
      Then, we want the total accumalated data to be as small as possible.
      <ul>
        <li>
          <p>
            <m>SS_X=\sum(x_i-\bar{x})^2=\sum x_i^2-\frac{(\sum x_i)^2}{n}</m>
          </p>
        </li>
        <li>
          <p>
            <m>SS_Y=\sum(y_i-\bar{y})^2=\sum y_i^2-\frac{(\sum y_i)^2}{n}</m>
          </p>
        </li>
        <li>
          <p>
            <m>SS_{XY}=\sum(x_i-\bar{x})(y_i-\bar{y})=\sum x_iy_i-\frac{(\sum x_i)(\sum y_i)}{n}</m>
          </p>
        </li>
      </ul>
    </p>
    <p>
      The slope of the regression line will be:
      <me>
        \beta_1:=\frac{SS_{XY}}{SS_{X}}
      </me>
    </p>
    <p>
      If we recall,
      a line needs not just a slope, but a <m>y</m> intercept.
      So how can we figure out what the <m>y</m> intercept should be?
      Well, given the slope <m>\beta_1</m>, ON AVERAGE, given some <m>x_i</m>,
      we should get that <m>y_i=\beta_1x_i+\beta_0</m>,
      where <m>\beta_0</m> is our yet-unknown <m>y</m>-intercept.
      Again, this won't happen every time, or possibly ever,
      but it should be the average result,
      so the way we find <m>\beta_0</m> is:
      <md>
        <mrow>\bar{y}\amp = \beta_1\bar{x}+\beta_0</mrow>
        <mrow>\beta_0\amp = \bar{y}-\beta_1\bar{x}</mrow>
      </md>.
    </p>
    <p>
      The above constructions are necessary to produce the best fit line.
      In order to measure how good of a fit this line is,
      we introduce the <term>correlation coefficient</term>,
      usually denoted <m>r</m>.
      This value is computed as follows:
      <me>
        r=\frac{n(\sum x_iy_i)-(\sum x_i)(\sum y_i)}{\sqrt{(n\sum x_i^2) - (\sum x_i)^2}\cdot \sqrt{n(\sum y_i^2)-(\sum y_i)^2}}
      </me>.
    </p>
    <p>
      This value measures the correction between the <m>x</m> and <m>y</m> values,
      with <m>r</m> ranging from <m>-1</m> to <m>1</m>.
      The way we should interpret the <m>r</m> values is as follows:
      <ol>
        <li>
          <p>
            Positive <m>r</m> means positive correlation,
            meaning as <m>x</m> goes up, <m>y</m> tends to go up.
            Negative <m>r</m> means the opposite,
            as <m>x</m> goes up, <m>y</m> tends to go down.
            Correlation of 0 means <m>x</m>'s change does not predict whether or not <m>y</m> goes up or down.
          </p>
        </li>
        <li>
          <p>
            The closer <m>r</m> is to 0, the weaker the correlation,
            meaning the ability to predict <m>y</m> from <m>x</m> is weak,
            the closer to <m>-1</m> or <m>1</m>,
            the better our ability to predict <m>y</m> from <m>x</m>.
          </p>
        </li>
      </ol>
    </p>
    <example xml:id="Example_visualizecorr">
      <title>Visualizations of different levels of correlation</title>
      <statement>
        <p>
          Consider the following plots of points.
          Whether the points tend to have an upward or downward trend determines the sign of <m>r</m>,
          how well the best fit line predicts the points determines how close <m>r</m> is to either <m>-1</m> or <m>1</m>.
        </p>
        <image>
<latex-image>
<![CDATA[\begin{tikzpicture}[scale=.3][domain=-1:7]
\draw[gray!50, thin, step=1] (-1,-1) grid (7,7);
\draw[very thick,->] (-1,0) -- (7.1,0) node[right] {\(x\)};
\draw[very thick,->] (0,-1) -- (0,7.1) node[above] {\(y\)};
\draw[blue, fill] (1,5) circle (3pt);
\draw[blue, fill] (2,3) circle (3pt);
\draw[blue, fill] (3,3) circle (3pt);
\draw[blue, fill] (4,2) circle (3pt);
\draw[blue, fill] (5,1) circle (3pt);
\draw[scale=1,domain=-1:6.111,smooth,variable=\x,red, dashed] plot ({\x},{-0.9*\x+5.5});
\draw (3, -2) --node{\(r=-0.9594\)} (3,-2);\end{tikzpicture}]]>
</latex-image>
        </image>
        <image>
<latex-image>
<![CDATA[\begin{tikzpicture}[scale=.3][domain=-1:7]
\draw[gray!50, thin, step=1] (-1,-1) grid (7,7);
\draw[very thick,->] (-1,0) -- (7.1,0) node[right] {\(x\)};
\draw[very thick,->] (0,-1) -- (0,7.1) node[above] {\(y\)};
\draw[blue, fill] (1,3) circle (3pt);
\draw[blue, fill] (2,3) circle (3pt);
\draw[blue, fill] (3,2) circle (3pt);
\draw[blue, fill] (4,2) circle (3pt);
\draw[blue, fill] (5,3) circle (3pt);
\draw[scale=1,domain=-1:7,smooth,variable=\x,red, dashed] plot ({\x},{-0.1*\x+2.9});
\draw (3, -2) --node{\(r=-0.2887\)} (3,-2);\end{tikzpicture}]]>
</latex-image>
        </image>
        <image>
<latex-image>
<![CDATA[\begin{tikzpicture}[scale=.3][domain=-1:7]
\draw[gray!50, thin, step=1] (-1,-1) grid (7,7);
\draw[very thick,->] (-1,0) -- (7.1,0) node[right] {\(x\)};
\draw[very thick,->] (0,-1) -- (0,7.1) node[above] {\(y\)};
\draw[blue, fill] (1,3) circle (3pt);
\draw[blue, fill] (2,4) circle (3pt);
\draw[blue, fill] (3,2) circle (3pt);
\draw[blue, fill] (4,4) circle (3pt);
\draw[blue, fill] (5,3) circle (3pt);
\draw[scale=1,domain=-1:7,smooth,variable=\x,red, dashed] plot ({\x},{3.2});
\draw (3, -2) --node{\(r=0\)} (3,-2);\end{tikzpicture}]]>
</latex-image>
        </image>
        <image>
<latex-image>
<![CDATA[\begin{tikzpicture}[scale=.3][domain=-1:7]
\draw[gray!50, thin, step=1] (-1,-1) grid (7,7);
\draw[very thick,->] (-1,0) -- (7.1,0) node[right] {\(x\)};
\draw[very thick,->] (0,-1) -- (0,7.1) node[above] {\(y\)};
\draw[blue, fill] (1,2) circle (3pt);
\draw[blue, fill] (2,5) circle (3pt);
\draw[blue, fill] (3,1) circle (3pt);
\draw[blue, fill] (4,4) circle (3pt);
\draw[blue, fill] (5,3) circle (3pt);
\draw[scale=1,domain=-1:7,smooth,variable=\x,red, dashed] plot ({\x},{0.1*\x+2.7});
\draw (3, -2) --node{\(r=0.1\)} (3,-2);\end{tikzpicture}]]>
</latex-image>
        </image>
        <image>
<latex-image>
<![CDATA[\begin{tikzpicture}[scale=.3][domain=-1:7]
\draw[gray!50, thin, step=1] (-1,-1) grid (7,7);
\draw[very thick,->] (-1,0) -- (7.1,0) node[right] {\(x\)};
\draw[very thick,->] (0,-1) -- (0,7.1) node[above] {\(y\)};
\draw[blue, fill] (1,2) circle (3pt);
\draw[blue, fill] (2,3) circle (3pt);
\draw[blue, fill] (3,3) circle (3pt);
\draw[blue, fill] (4,5) circle (3pt);
\draw[blue, fill] (5,6) circle (3pt);
\draw[scale=1,domain=-1:6.2,smooth,variable=\x,red, dashed] plot ({\x},{\x+0.8});
\draw (3, -2) --node{\(r=0.9623\)} (3,-2);\end{tikzpicture}]]>
</latex-image>
        </image>
        <me>
          <image>
<latex-image>
<![CDATA[\begin{tikzpicture}[scale=7]
\draw (-1,0)--(1,0);
\draw (-1,0)--node[below left]{$-1$}(-1,0);
\draw (1,0)--node[below right]{$1$}(1,0);
\draw[red]( 0.9623,0)--node{$*$}( 0.9623,0);
\draw( 0.9623,0)--node[above]{$r=0.9623$}( 0.9623,0);
\draw[red]( 0.1,0)--node{$*$}( 0.1,0);
\draw( 0.1,0)--node[above]{$r=0.1$}( 0.1,0);
\draw[red]( 0.0,0)--node{$*$}( 0.0,0);
\draw( 0.0,0)--node[below]{$r=0$}( 0.0,0);
\draw[red]( -0.2887,0)--node{$*$}( -0.2887,0);
\draw( -0.2887,0)--node[above]{$r=-0.2887$}( -0.2887,0);
\draw[red]( -0.9594,0)--node{$*$}( -0.9594,0);
\draw( -0.9594,0)--node[above]{$r=-0.9594$}( -0.9594,0);\end{tikzpicture}]]>
</latex-image>
          </image>
        </me>
      </statement>
    </example>
    <p>
      We further note that the closer <m>r</m> is to either <m>-1, 1</m> the closer <m>r^2</m> is to 1, and similarly the closer <m>r</m> is to 0, the closer <m>r^2</m> is to 1.
      Thus, we use <m>r^2</m> as an overall measure of the strength of the correlation.
      In fact, we say that <m>y</m> is <m>r^2</m> predicted by <m>x</m>,
      that is the values of <m>y</m> are <m>r^2</m> determinable by the values of <m>x</m>,
      the rest by other, as of yet undetermined variables.
    </p>
    <example xml:id="Example_BasicRegressionContd">
      <title>Finding a best-fit line continued: correlation</title>
      <statement>
        <p>
          We continue from <xref ref="Example_BasicRegression">Example</xref>:
          <md>
            <mrow>\bar{x}\amp = \frac{1+2+3+4+5}{5}=3</mrow>
            <mrow>\bar{y}\amp = \frac{2+2+4+4+6}{5}=3.6</mrow>
            <mrow>SS_X\amp = (1-3)^2+(2-3)^2+(3-3)^2+(4-3)^2+(5-3)^2=10</mrow>
            <mrow>SS_Y\amp = (2-3.6)^2+(2-3.6)^2+(4-3.6)^2+(4-3.6)^2+(6-3.6)^2=11.2</mrow>
            <mrow>SS_{XY}\amp = (1-3)(2-3.6)+(2-3)(2-3.6)+(3-3)(4-3.6)+</mrow>
            <mrow>\amp \ \  (4-3)(4-3.6)+(5-3)(6-3.6)=10</mrow>
            <mrow>\beta_1\amp = \frac{10}{10}=1</mrow>
            <mrow>\beta_0\amp = 3.6-(1)(3)=0.6</mrow>
          </md>
        </p>
        <p>
          So our best fit line is <m>y=1x+0.6</m>.
        </p>
        <image>
<latex-image>
<![CDATA[\begin{tikzpicture}[scale=.6][domain=-1:7]
\draw[gray!50, thin, step=1] (-1,-1) grid (7,7);
\draw[very thick,->] (-1,0) -- (7.1,0) node[right] {\(x\)};
\draw[very thick,->] (0,-1) -- (0,7.1) node[above] {\(y\)};
\draw[blue, fill] (1,2) circle (3pt);
\draw[blue, fill] (2,2) circle (3pt);
\draw[blue, fill] (3,4) circle (3pt);
\draw[blue, fill] (4,4) circle (3pt);
\draw[blue, fill] (5,6) circle (3pt);
\draw[scale=1,domain=-1:6.4,smooth,variable=\x,red] plot ({\x},{\x+0.6}) node[above]{\(y=x+0.6\)};\end{tikzpicture}]]>
</latex-image>
        </image>
        <p>
          Again, notice that none of the points actually fall on this line,
          but no possible line could have accomplished all of the points falling on it.
          What we have instead is a line that passes through the points as closely as possible.
          Define <m>e_i</m> to be the error in prediction for <m>y_i</m>,
          that is let <m>e_i=y_i-(1\cdot x_i+0.6)</m>, then we see that:
          <md>
            <mrow>e_1\amp = 2-(1+.6)=0.4</mrow>
            <mrow>e_2\amp = 2-(2+.6)=-0.6</mrow>
            <mrow>e_3\amp = 4-(3+.6)=0.4</mrow>
            <mrow>e_4\amp = 4-(4+.6)=-0.6</mrow>
            <mrow>e_5\amp = 6-(5+.6)=0.4</mrow>
          </md>
        </p>
        <image>
<latex-image>
<![CDATA[\begin{tikzpicture}[scale=.6][domain=-1:7]
\draw[gray!50, thin, step=1] (-1,-1) grid (7,7);
\draw[very thick,->] (-1,0) -- (7.1,0) node[right] {\(x\)};
\draw[very thick,->] (0,-1) -- (0,7.1) node[above] {\(y\)};
\draw[blue, fill] (1,2) circle (3pt);
\draw[blue, fill] (2,2) circle (3pt);
\draw[blue, fill] (3,4) circle (3pt);
\draw[blue, fill] (4,4) circle (3pt);
\draw[blue, fill] (5,6) circle (3pt);
\draw[scale=1,domain=-1:6.4,smooth,variable=\x,red] plot ({\x},{\x+0.6}) node[above]{\(y=x+0.6\)};
\draw[blue, thick](1, 1.6) -- (1,2);
\draw[blue, thick](2, 2.6) -- (2,2);
\draw[blue, thick](3, 3.6) -- (3,4);
\draw[blue, thick](4, 4.6) -- (4,4);
\draw[blue, thick](5, 5.6) -- (5,6);\end{tikzpicture}]]>
</latex-image>
        </image>
        <p>
          But
          <me>
            e_1+e_2+e_3+e_4+e_5=(0.4)+(-0.6)+(0.4)+(0.6)+(0.4)=0
          </me>.
        </p>
        <p>
          This tells us this line passes through the
          <q>middle</q>
          of the points.
          Is it the best possible line?
          Notice that
          <me>
            \sum e_i^2=(0.4)^2+(-0.6)^2+(0.4)^2+(0.6)^2+(0.4)^2=1.2
          </me>.
        </p>
        <p>
          Is this the best we can do?
          Again, a formal verification of this is beyond this text,
          but following this link: \url{https://www.desmos.com/calculator/gruczlkyyf}, you can see that by adjusting the parameters <m>m</m> and <m>b</m>,
          that the sum of squares being 1.2 is the smallest error sum you can achieve.
        </p>
        <p>
          To find <m>r</m> we note that:
          <md>
            <mrow>r\amp = \frac{n(\sum x_iy_i)-(\sum x_i)(\sum y_i)}{\sqrt{(n\sum x_i^2) - (\sum x_i)^2}\cdot \sqrt{n(\sum y_i^2)-(\sum y_i)^2}}</mrow>
            <mrow>\amp = \frac{5(64)-(15)(18)}{\sqrt{5(55) - (15)^2}\cdot \sqrt{5(76)-(18)^2}}</mrow>
            <mrow>\amp \approx 0.9449</mrow>
          </md>.
        </p>
        <p>
          So we can see that this fit is fairly close,
          and <m>x</m> is a good predictor of <m>y</m>.
          Since <m>r^2\approx 0.8929</m>,
          we can say that <m>y</m>'s value are 89.29% determinable by <m>x</m>'s values.
        </p>
      </statement>
    </example>
  </subsection>
  <subsection>
    <title>Good thing we don't live in the stone age.</title>
    <p>
      We should all agree that this was quite a lot of work to find these parameters for 5 values,
      and most data sets of work,
      have dozens, if not hundreds or thousands of data points.
      It's simply impractical to do this by hand.
    </p>
    <p>
      On the other hand a quick use of Desmos and entering the data points into the table,
      produces the best fit line, a plot,
      and the correlation coefficients.
      <interactive desmos="ir10f1wwzr"  />
    </p>
    <p>
      Try entering your own data, adding or removing rows,
      and see how quickly and readily it will obtain our best fit lines and correlation coefficients!
    </p>
  </subsection>
  <subsection>
    <title>Applications</title>
    <example>
      <title>Linear Regression with Murder Rate and Time</title>
      <statement>
        <p>
          From fbi.gov from 2004-2013,
          the murder rates per 100,000 people are listed below:
          <me>
            \begin{array}{c|c} \text{ Years after 2000 }  \amp  \text{ Murders per 100,000 people } \\ x\amp y\\ \hline 4\amp 5.5\\ 5\amp 5.6\\ 6\amp 5.8\\ 7\amp 5.7\\ 8\amp 5.4\\ 9\amp 5\\ 10\amp 4.8\\ 11\amp 4.7\\ 12\amp 4.7\\ 13\amp 4.5 \end{array}
          </me>
          <ol>
            <li>
              <p>
                Determine the best fit line for murders (per 100,000 people) <m>x</m> years after 2000.
              </p>
            </li>
            <li>
              <p>
                How much of a factor is the year in predicting the number of murders?
              </p>
            </li>
            <li>
              <p>
                Predict the murder rate in 2018.
              </p>
            </li>
          </ol>
        </p>
        </statement>
        <solution>
        <p>
          Entering this data in desmos: 
          <interactive desmos="em6xkq7udn"  />
          
          <ol>
            <li>
              <p>
                It seems that the best fit line is <m>y=-0.144848x+6.40121</m>.
              </p>
            </li>
            <li>
              <p>
                Since <m>r^2=0.8318</m>,
                we can say the murder rate is 83.18% predictable by the year.
              </p>
            </li>
            <li>
              <p>
                In 2018, the murder rate should be
                <m>-0.144848(18)+6.40121=3.793946</m> or roughly 3.8 murders per 100,000 people in 2018.
              </p>
            </li>
          </ol>
        </p>
      </solution>
    </example>
    <example xml:id="Example_CorrQuizTest">
      <title>Linear Regression between Quiz and Test scores</title>
      <statement>
        <p>
          I personally wondered in quiz scores are a good predictor of test scores in Calculus.
          I pulled out some data from this semester,
          and here are the average quiz and test scores of my students,
          anonymously of course:
          <me>
            \begin{array}{c|c} \text{ Quiz Average }  \amp  \text{ Test Average } \\ x\amp y\\ \hline 87.5\amp 100\\ 76.13\amp 94\\ 75.83\amp 93.5\\ 53.53\amp 83.5\\ 76.13\amp 83\\ 71.13\amp 78.85\\ 56.13\amp 76.5\\ 73.57\amp 70\\ 66.13\amp 69.5\\ 66.13\amp 63 \end{array}
          </me>

          <ol>
      <li>
        <p>
          Find the linear relationship between Quiz grades and Test grades.
        </p>
      </li>
      <li>
        <p>
          Are Quiz grades a good predictor of Test grades?
        </p>
      </li>
      <li>
        <p>
          If someone has an 80 average on Quizzes, predict their Test grade.
        </p>
      </li>
    </ol>
        </p>
      </statement>
    
    
      <solution>
        <p>
      So, once again: 
      <interactive desmos="atipunytb1"  />
      
      <ol>
        <li>
          <p>
            The relationship here is: <m>y=0.639883x+36.2168</m>,
            so there is a positive correlation, which we expect.
          </p>
        </li>
        <li>
          <p>
            <m>r^2=0.2906</m>, so Test grades 29.06% predictable by Quiz grades,
            which one would also expect,
            some people perform better in the Exam situation and some people perform worse.
          </p>
        </li>
        <li>
          <p>
            We would have an expected test grade of
            <m>y=0.639883(80)+36.2168=87.40744</m> or about 87.41%. But of course with an <m>r^2</m> that small we shouldn't expect this to be terribly accurate.
          </p>
        </li>
      </ol>
    </p>
  </solution>
</example>


      <exercise>
        <title>Soybean production</title>
      <statement>
        <p>
          TThe following table
shows soybean production, in millions of tons, in the United
States as a function of the cultivated area, in millions of acres:
          <me>
            \begin{array}{c|c} \text{ Area (millions of acres) }  \amp  \text{ Production (millions of tons) } \\ x\amp y\\ 
            \hline 
            30\amp 20\\ 
            42\amp 33\\ 
            69\amp 55\\ 
            59\amp 57\\ 
            74\amp 83\\ 
           74\amp 88\\
             \end{array}
          </me>
          <ol>
            <li>
              <p>
                Determine the best fit line of production as a function of area.
              </p>
            </li>
            <li>
              <p>
              Find the correlation coefficient, what does it tell us?
            </p>
            </li>
            <li>
              <p>
                Interpret the slope of the regression line
              </p>
            </li>
            <li>
              <p>
                Predict the production (in millions of tons) if 65 million acres are dedicated to soybean farming.
              </p>
            </li>
            <li>
              <p>
                Predict the area (in millions of acres) used to farm soybeans if 70 million tons are produced.
              </p>
            </li>
          </ol>
        </p>
        </statement>
        <solution>
        <p>
          Entering this data in desmos: 
          <interactive desmos="gdszirroq1"  />
          
          <ol>
            <li>
              <p>
                It seems that the best fit line is <m>y=1.37993x-24.0358</m>.
              </p>
            </li>
            <li>
              <p>
              We have that <m>r=0.9436</m> so a strong positive relationship.
            </p>
            </li>
            <li>
              <p>
                Units of <m>y</m> are g, units of <m>x</m> are inkg, so the slope represents about an increase of 3.85777 grams in a cats heart's weight per kg of the cat's weight.
              </p>
            </li>
            <li>
              <p>
                So 65 million acres means <m>x=65</m> and 
                <m>y=1.37993(65)-24.0358=65.65965</m> or roughly 65.66 million tons.  So 65 million acres of farming produces on average 65.66 tons of soybeans.
              </p>
            </li>
            <li>
              <p>
                If 70 million tons are produced, then we have <m>y=70</m>. So we have:
                <md>
                  <mrow>y\amp= 1.37993x-24.0358</mrow>
                  <mrow>70\amp= 1.37993x-24.0358</mrow>
                  <mrow>1.37993x\amp=94.0358</mrow>
                  <mrow>x\amp=\frac{94.0358}{1.37993}\approx 68.145</mrow>
                </md>  
              So if 70 million tons of soybeans are produced, it on average would take 68.145 million acres to produce.
              </p>
            </li>
          </ol>
        </p>
      </solution>
    </exercise>


      <exercise>
        <title>Cat weights and their hearts</title>
      <statement>
        <p>
          The following table
shows the weight of cats in kg and their hearts in grams.  Fifteen cats were measured:
          <me>
            \begin{array}{c|c} \text{ Cat weight (kg) }  \amp  \text{ Heart weight (g) } \\ x\amp y\\ \hline 2.077\amp 7.955\\ 
            2.110\amp 8.539\\ 
            3.572\amp 14.011\\ 
            2.603\amp 10.449\\ 
            3.067\amp 11.942\\ 
            3.631\amp 14.454\\
            3.080\amp 12.001\\
            2.584\amp 10.062\\
            3.085\amp 12.115\\
            2.769\amp 10.853\\
            3.678\amp 14.653\\
            3.048\amp 12.131\\
            3.810\amp 15.079\\
            2.756\amp 10.724\\
             \end{array}
          </me>
          <ol>
            <li>
              <p>
                Determine the best fit line of Heart weight as a function of cat weight.
              </p>
            </li>
            <li>
              <p>
              Find the correlation coefficient, what does it tell us?
            </p>
            </li>
            <li>
              <p>
                Interpret the slope of the regression line
              </p>
            </li>
            <li>
              <p>
                Predict the weight of a cats heart (in grams) if the cat weight 3.5 kg.
              </p>
            </li>
            <li>
              <p>
                Predict the weight of a cat (in kg) if the cat's heart weighs 13 g.
              </p>
            </li>
          </ol>
        </p>
        </statement>
        <solution>
        <p>
          Entering this data in desmos: 
          <interactive desmos="ewrufu6pfg"  />
          
          <ol>
            <li>
              <p>
                It seems that the best fit line is <m>y=3.85777x+0.234354</m>.
              </p>
            </li>
            <li>
              <p>
              We have that <m>r=0.9969</m> so an extremely strong positive relationship.
            </p>
            </li>
            <li>
              <p>
                Units of <m>y</m> are g, units of <m>x</m> are inkg, so the slope represents about an increase of 3.85777 grams in a cats heart's weight per kg of the cat's weight.
              </p>
            </li>
            <li>
              <p>
                So 3.5 kg means <m>x=3.5</m> and 
                <m>y=3.85777(3.5)+0.234354=13.736549</m> or roughly 13.737 g.  So a cat weighing 3.5 kg would have a heart that weighs about 13.737 grams.
              </p>
            </li>
            <li>
              <p>
                If the cat's heart weighs 13 g, then we have <m>y=13</m>. So we have:
                <md>
                  <mrow>y\amp= 3.85777x+0.234354</mrow>
                  <mrow>13\amp= 3.85777x+0.234354</mrow>
                  <mrow>3.85777x\amp=13-0.234354</mrow>
                  <mrow>3.85777x\amp=12.765646</mrow>
                  <mrow>x\amp=\frac{12.765646}{3.85777}\approx 3.309</mrow>
                </md>  
              So a cat whose heart weighs 13 g most likely weighs about 3.309 kg.
              </p>
            </li>
          </ol>
        </p>
      </solution>
    </exercise>





  </subsection>
  <subsection>
    <title>Correlation <m>\neq</m> Causation!</title>
    <p>
      A common misconception about correlated variables is that a strong correlation means that the <m>x</m> variables
      <term>causes</term> the <m>y</m> variable.
      This is sometimes the case, but very often not true.
    </p>
    <p>
      It is just as possible that <m>y</m> causes <m>x</m>,
      or that <m>x</m> and <m>y</m> <term>share</term> common causes,
      or there's always complete coincidence.
    </p>
    <p>
      In <xref ref="Example_CorrQuizTest">Example</xref>,
      if I had simply given everyone 100 on the quizzes without looking at them,
      and graded the exams in normal way,
      they likely would not have done any better.
      One could argue that they would have done worse as a result.
      So a positive correlation between the variables doesn't mean that one will rise simply by virtue of the other rising.
    </p>
    <p>
      The bottom line is that correlations measure the strength and form of a relationship between variables,
      but not the nature of the relationship.
    </p>
  </subsection>
</section>